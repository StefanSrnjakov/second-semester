{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.19      0.23      0.21       151\n",
      "ARTS & CULTURE       0.32      0.22      0.26       134\n",
      "  BLACK VOICES       0.47      0.50      0.48       458\n",
      "      BUSINESS       0.39      0.46      0.43       599\n",
      "       COLLEGE       0.34      0.34      0.34       114\n",
      "        COMEDY       0.43      0.44      0.44       540\n",
      "         CRIME       0.52      0.51      0.51       356\n",
      "CULTURE & ARTS       0.32      0.22      0.26       107\n",
      "       DIVORCE       0.66      0.68      0.67       343\n",
      "     EDUCATION       0.31      0.31      0.31       101\n",
      " ENTERTAINMENT       0.67      0.66      0.66      1736\n",
      "   ENVIRONMENT       0.34      0.26      0.29       144\n",
      "         FIFTY       0.17      0.19      0.18       140\n",
      "  FOOD & DRINK       0.59      0.62      0.60       634\n",
      "     GOOD NEWS       0.21      0.18      0.19       140\n",
      "         GREEN       0.33      0.39      0.36       262\n",
      "HEALTHY LIVING       0.25      0.29      0.26       670\n",
      " HOME & LIVING       0.65      0.62      0.64       432\n",
      "        IMPACT       0.22      0.31      0.26       348\n",
      " LATINO VOICES       0.45      0.31      0.37       113\n",
      "         MEDIA       0.41      0.41      0.41       294\n",
      "         MONEY       0.37      0.36      0.37       176\n",
      "     PARENTING       0.46      0.50      0.48       879\n",
      "       PARENTS       0.33      0.31      0.32       396\n",
      "      POLITICS       0.79      0.73      0.76      3560\n",
      "  QUEER VOICES       0.69      0.67      0.68       635\n",
      "      RELIGION       0.47      0.52      0.50       258\n",
      "       SCIENCE       0.51      0.45      0.48       221\n",
      "        SPORTS       0.63      0.66      0.65       508\n",
      "         STYLE       0.29      0.20      0.24       225\n",
      "STYLE & BEAUTY       0.72      0.73      0.73       982\n",
      "         TASTE       0.31      0.25      0.28       210\n",
      "          TECH       0.44      0.42      0.43       210\n",
      " THE WORLDPOST       0.44      0.37      0.40       366\n",
      "        TRAVEL       0.67      0.69      0.68       990\n",
      "     U.S. NEWS       0.23      0.13      0.17       138\n",
      "      WEDDINGS       0.75      0.72      0.74       365\n",
      "    WEIRD NEWS       0.27      0.28      0.28       278\n",
      "      WELLNESS       0.55      0.54      0.55      1795\n",
      "         WOMEN       0.27      0.34      0.30       357\n",
      "    WORLD NEWS       0.38      0.38      0.38       330\n",
      "     WORLDPOST       0.25      0.29      0.27       258\n",
      "\n",
      "      accuracy                           0.54     20953\n",
      "     macro avg       0.43      0.42      0.42     20953\n",
      "  weighted avg       0.55      0.54      0.54     20953\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# === NLTK setup ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# === Step 1: Load JSON lines ===\n",
    "with open('news_categories.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline', 'category']].dropna()\n",
    "\n",
    "# === Step 2: Preprocess headlines ===\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean'] = df['headline'].apply(preprocess)\n",
    "\n",
    "# === Step 3: Encode categories ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: Build pipeline with TF-IDF and LinearSVC ===\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        LinearSVC(class_weight='balanced', max_iter=5000)\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Step 6: Train and evaluate ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# precision    recall  f1-score   support\n",
    "\n",
    "#           ARTS       0.19      0.23      0.21       151\n",
    "# ARTS & CULTURE       0.32      0.22      0.26       134\n",
    "#   BLACK VOICES       0.47      0.50      0.48       458\n",
    "#       BUSINESS       0.39      0.46      0.43       599\n",
    "#        COLLEGE       0.34      0.34      0.34       114\n",
    "#         COMEDY       0.43      0.44      0.44       540\n",
    "#          CRIME       0.52      0.51      0.51       356\n",
    "# CULTURE & ARTS       0.32      0.22      0.26       107\n",
    "#        DIVORCE       0.66      0.68      0.67       343\n",
    "#      EDUCATION       0.31      0.31      0.31       101\n",
    "#  ENTERTAINMENT       0.67      0.66      0.66      1736\n",
    "#    ENVIRONMENT       0.34      0.26      0.29       144\n",
    "#          FIFTY       0.17      0.19      0.18       140\n",
    "#   FOOD & DRINK       0.59      0.62      0.60       634\n",
    "#      GOOD NEWS       0.21      0.18      0.19       140\n",
    "#          GREEN       0.33      0.39      0.36       262\n",
    "# HEALTHY LIVING       0.25      0.29      0.26       670\n",
    "#  HOME & LIVING       0.65      0.62      0.64       432\n",
    "#         IMPACT       0.22      0.31      0.26       348\n",
    "#  LATINO VOICES       0.45      0.31      0.37       113\n",
    "#          MEDIA       0.41      0.41      0.41       294\n",
    "# ...\n",
    "#       accuracy                           0.54     20953\n",
    "#      macro avg       0.43      0.42      0.42     20953\n",
    "#   weighted avg       0.55      0.54      0.54     20953\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.13      0.17      0.14       302\n",
      "ARTS & CULTURE       0.17      0.12      0.14       268\n",
      "  BLACK VOICES       0.23      0.24      0.23       917\n",
      "      BUSINESS       0.20      0.28      0.23      1198\n",
      "       COLLEGE       0.26      0.17      0.21       229\n",
      "        COMEDY       0.15      0.22      0.18      1080\n",
      "         CRIME       0.38      0.26      0.30       712\n",
      "CULTURE & ARTS       0.24      0.20      0.22       215\n",
      "       DIVORCE       0.45      0.32      0.37       685\n",
      "     EDUCATION       0.22      0.20      0.21       203\n",
      " ENTERTAINMENT       0.42      0.45      0.43      3473\n",
      "   ENVIRONMENT       0.28      0.19      0.23       289\n",
      "         FIFTY       0.06      0.09      0.07       280\n",
      "  FOOD & DRINK       0.45      0.38      0.41      1268\n",
      "     GOOD NEWS       0.16      0.12      0.14       280\n",
      "         GREEN       0.28      0.24      0.26       524\n",
      "HEALTHY LIVING       0.13      0.19      0.16      1339\n",
      " HOME & LIVING       0.52      0.34      0.41       864\n",
      "        IMPACT       0.11      0.13      0.12       697\n",
      " LATINO VOICES       0.21      0.15      0.18       226\n",
      "         MEDIA       0.27      0.31      0.29       589\n",
      "         MONEY       0.26      0.24      0.25       351\n",
      "     PARENTING       0.16      0.19      0.17      1758\n",
      "       PARENTS       0.18      0.15      0.17       791\n",
      "      POLITICS       0.56      0.43      0.49      7121\n",
      "  QUEER VOICES       0.41      0.34      0.37      1269\n",
      "      RELIGION       0.29      0.29      0.29       515\n",
      "       SCIENCE       0.34      0.27      0.30       441\n",
      "        SPORTS       0.52      0.41      0.46      1015\n",
      "         STYLE       0.14      0.25      0.18       451\n",
      "STYLE & BEAUTY       0.51      0.32      0.39      1963\n",
      "         TASTE       0.23      0.22      0.22       419\n",
      "          TECH       0.31      0.22      0.26       421\n",
      " THE WORLDPOST       0.30      0.21      0.25       733\n",
      "        TRAVEL       0.42      0.45      0.43      1980\n",
      "     U.S. NEWS       0.10      0.12      0.10       275\n",
      "      WEDDINGS       0.60      0.32      0.42       731\n",
      "    WEIRD NEWS       0.16      0.19      0.17       555\n",
      "      WELLNESS       0.26      0.36      0.30      3589\n",
      "         WOMEN       0.18      0.24      0.20       714\n",
      "    WORLD NEWS       0.26      0.21      0.23       660\n",
      "     WORLDPOST       0.17      0.21      0.18       516\n",
      "\n",
      "      accuracy                           0.32     41906\n",
      "     macro avg       0.28      0.25      0.26     41906\n",
      "  weighted avg       0.35      0.32      0.33     41906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from scipy.sparse import hstack, identity\n",
    "from scipy.sparse.linalg import lsqr\n",
    "\n",
    "# === NLTK setup ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# === Step 1: Load JSON lines ===\n",
    "with open('news_categories.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline', 'category']].dropna()\n",
    "\n",
    "# === Step 2: Preprocess headlines ===\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean'] = df['headline'].apply(preprocess)\n",
    "\n",
    "# === Step 3: Encode categories ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: Custom Sparse-Compatible LS-TWSVM ===\n",
    "class LSTWSVM(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, c1=0.01, c2=0.01):\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.where(y == 1, 1, -1)\n",
    "        X1 = X[y == 1]\n",
    "        X2 = X[y == -1]\n",
    "\n",
    "        e1 = np.ones((X1.shape[0], 1))\n",
    "        e2 = np.ones((X2.shape[0], 1))\n",
    "\n",
    "        A = hstack([X1, e1])\n",
    "        B = hstack([X2, e2])\n",
    "        I = identity(A.shape[1])\n",
    "\n",
    "        # Solve least squares with regularization\n",
    "        lhs1 = B.T @ B + self.c1 * I\n",
    "        rhs1 = B.T @ e2\n",
    "        self.w1 = lsqr(lhs1, rhs1)[0]\n",
    "\n",
    "        lhs2 = A.T @ A + self.c2 * I\n",
    "        rhs2 = A.T @ e1\n",
    "        self.w2 = lsqr(lhs2, rhs2)[0]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        bias = np.ones((X.shape[0], 1))\n",
    "        X_aug = hstack([X, bias])\n",
    "        f1 = np.abs(X_aug @ self.w1)\n",
    "        f2 = np.abs(X_aug @ self.w2)\n",
    "        return f2 - f1\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(self.decision_function(X) < 0, 1, 0)\n",
    "\n",
    "# === Step 6: Build Pipeline with TF-IDF and One-vs-Rest ===\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(LSTWSVM()))\n",
    "])\n",
    "\n",
    "# === Step 7: Train and evaluate ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "#                 precision    recall  f1-score   support\n",
    "\n",
    "#           ARTS       0.13      0.17      0.14       302\n",
    "# ARTS & CULTURE       0.17      0.12      0.14       268\n",
    "#   BLACK VOICES       0.23      0.24      0.23       917\n",
    "#       BUSINESS       0.20      0.28      0.23      1198\n",
    "#        COLLEGE       0.26      0.17      0.21       229\n",
    "#         COMEDY       0.15      0.22      0.18      1080\n",
    "#          CRIME       0.38      0.26      0.30       712\n",
    "# CULTURE & ARTS       0.24      0.20      0.22       215\n",
    "#        DIVORCE       0.45      0.32      0.37       685\n",
    "#      EDUCATION       0.22      0.20      0.21       203\n",
    "#  ENTERTAINMENT       0.42      0.45      0.43      3473\n",
    "#    ENVIRONMENT       0.28      0.19      0.23       289\n",
    "#          FIFTY       0.06      0.09      0.07       280\n",
    "#   FOOD & DRINK       0.45      0.38      0.41      1268\n",
    "#      GOOD NEWS       0.16      0.12      0.14       280\n",
    "#          GREEN       0.28      0.24      0.26       524\n",
    "# HEALTHY LIVING       0.13      0.19      0.16      1339\n",
    "#  HOME & LIVING       0.52      0.34      0.41       864\n",
    "#         IMPACT       0.11      0.13      0.12       697\n",
    "#  LATINO VOICES       0.21      0.15      0.18       226\n",
    "#          MEDIA       0.27      0.31      0.29       589\n",
    "# ...\n",
    "#       accuracy                           0.32     41906\n",
    "#      macro avg       0.28      0.25      0.26     41906\n",
    "#   weighted avg       0.35      0.32      0.33     41906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.42      0.21      0.28       302\n",
      "ARTS & CULTURE       0.35      0.15      0.21       268\n",
      "  BLACK VOICES       0.56      0.42      0.48       917\n",
      "      BUSINESS       0.50      0.44      0.47      1198\n",
      "       COLLEGE       0.52      0.45      0.48       229\n",
      "        COMEDY       0.60      0.41      0.49      1080\n",
      "         CRIME       0.51      0.54      0.52       712\n",
      "CULTURE & ARTS       0.59      0.22      0.32       215\n",
      "       DIVORCE       0.78      0.65      0.71       685\n",
      "     EDUCATION       0.47      0.32      0.38       203\n",
      " ENTERTAINMENT       0.62      0.74      0.67      3473\n",
      "   ENVIRONMENT       0.48      0.20      0.29       289\n",
      "         FIFTY       0.44      0.13      0.20       280\n",
      "  FOOD & DRINK       0.59      0.71      0.64      1268\n",
      "     GOOD NEWS       0.37      0.13      0.19       280\n",
      "         GREEN       0.40      0.32      0.36       524\n",
      "HEALTHY LIVING       0.34      0.15      0.21      1339\n",
      " HOME & LIVING       0.64      0.67      0.66       864\n",
      "        IMPACT       0.42      0.22      0.29       697\n",
      " LATINO VOICES       0.67      0.36      0.47       226\n",
      "         MEDIA       0.59      0.42      0.50       589\n",
      "         MONEY       0.48      0.31      0.38       351\n",
      "     PARENTING       0.49      0.59      0.53      1758\n",
      "       PARENTS       0.43      0.19      0.26       791\n",
      "      POLITICS       0.68      0.85      0.75      7121\n",
      "  QUEER VOICES       0.75      0.69      0.72      1269\n",
      "      RELIGION       0.61      0.55      0.57       515\n",
      "       SCIENCE       0.57      0.48      0.52       441\n",
      "        SPORTS       0.63      0.73      0.68      1015\n",
      "         STYLE       0.46      0.10      0.16       451\n",
      "STYLE & BEAUTY       0.66      0.80      0.72      1963\n",
      "         TASTE       0.31      0.11      0.17       419\n",
      "          TECH       0.55      0.50      0.52       421\n",
      " THE WORLDPOST       0.45      0.39      0.42       733\n",
      "        TRAVEL       0.62      0.77      0.69      1980\n",
      "     U.S. NEWS       0.48      0.08      0.14       275\n",
      "      WEDDINGS       0.77      0.78      0.77       731\n",
      "    WEIRD NEWS       0.33      0.20      0.25       555\n",
      "      WELLNESS       0.49      0.73      0.59      3589\n",
      "         WOMEN       0.44      0.28      0.34       714\n",
      "    WORLD NEWS       0.42      0.29      0.34       660\n",
      "     WORLDPOST       0.38      0.20      0.26       516\n",
      "\n",
      "      accuracy                           0.59     41906\n",
      "     macro avg       0.52      0.42      0.44     41906\n",
      "  weighted avg       0.57      0.59      0.56     41906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# === NLTK setup ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# === Step 1: Load JSON lines ===\n",
    "with open('news_categories.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline', 'category']].dropna()\n",
    "\n",
    "# === Step 2: Preprocess headlines ===\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean'] = df['headline'].apply(preprocess)\n",
    "\n",
    "# === Step 3: Encode categories ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: TF-IDF + RidgeClassifier (LS-SVM Approximation) ===\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        RidgeClassifier(alpha=1.0)\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Step 6: Train and evaluate ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.41      0.21      0.27       302\n",
      "ARTS & CULTURE       0.38      0.16      0.23       268\n",
      "  BLACK VOICES       0.55      0.42      0.48       917\n",
      "      BUSINESS       0.50      0.44      0.47      1198\n",
      "       COLLEGE       0.53      0.45      0.49       229\n",
      "        COMEDY       0.60      0.41      0.49      1080\n",
      "         CRIME       0.51      0.53      0.52       712\n",
      "CULTURE & ARTS       0.62      0.21      0.32       215\n",
      "       DIVORCE       0.77      0.63      0.69       685\n",
      "     EDUCATION       0.47      0.33      0.39       203\n",
      " ENTERTAINMENT       0.62      0.75      0.68      3473\n",
      "   ENVIRONMENT       0.47      0.19      0.27       289\n",
      "         FIFTY       0.44      0.15      0.23       280\n",
      "  FOOD & DRINK       0.58      0.71      0.64      1268\n",
      "     GOOD NEWS       0.32      0.12      0.17       280\n",
      "         GREEN       0.41      0.32      0.36       524\n",
      "HEALTHY LIVING       0.34      0.15      0.21      1339\n",
      " HOME & LIVING       0.64      0.67      0.66       864\n",
      "        IMPACT       0.43      0.23      0.30       697\n",
      " LATINO VOICES       0.64      0.35      0.45       226\n",
      "         MEDIA       0.58      0.44      0.50       589\n",
      "         MONEY       0.48      0.33      0.39       351\n",
      "     PARENTING       0.48      0.58      0.53      1758\n",
      "       PARENTS       0.40      0.18      0.24       791\n",
      "      POLITICS       0.68      0.85      0.76      7121\n",
      "  QUEER VOICES       0.76      0.69      0.72      1269\n",
      "      RELIGION       0.59      0.53      0.56       515\n",
      "       SCIENCE       0.57      0.50      0.53       441\n",
      "        SPORTS       0.64      0.73      0.68      1015\n",
      "         STYLE       0.46      0.10      0.17       451\n",
      "STYLE & BEAUTY       0.67      0.80      0.73      1963\n",
      "         TASTE       0.29      0.10      0.15       419\n",
      "          TECH       0.57      0.50      0.53       421\n",
      " THE WORLDPOST       0.45      0.39      0.42       733\n",
      "        TRAVEL       0.62      0.77      0.69      1980\n",
      "     U.S. NEWS       0.48      0.08      0.13       275\n",
      "      WEDDINGS       0.78      0.77      0.77       731\n",
      "    WEIRD NEWS       0.34      0.20      0.25       555\n",
      "      WELLNESS       0.49      0.73      0.59      3589\n",
      "         WOMEN       0.42      0.26      0.32       714\n",
      "    WORLD NEWS       0.42      0.29      0.34       660\n",
      "     WORLDPOST       0.38      0.20      0.26       516\n",
      "\n",
      "      accuracy                           0.59     41906\n",
      "     macro avg       0.52      0.42      0.44     41906\n",
      "  weighted avg       0.56      0.59      0.56     41906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# === NLTK setup ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# === Step 1: Load JSON lines ===\n",
    "with open('news_categories.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline', 'category']].dropna()\n",
    "\n",
    "# === Step 2: Preprocess headlines (with lemmatization) ===\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t.isalpha() and t not in stop_words and len(t) > 2]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['clean'] = df['headline'].apply(preprocess)\n",
    "\n",
    "# === Step 3: Encode categories ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# === Step 4: Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# === Step 5: TF-IDF + RidgeClassifier (LS-SVM Approximation) ===\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        RidgeClassifier(alpha=1.0)\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Step 6: Train and evaluate ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cecepasinechka/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          ARTS       0.39      0.14      0.21       302\n",
      "ARTS & CULTURE       0.39      0.17      0.24       268\n",
      "  BLACK VOICES       0.51      0.37      0.43       917\n",
      "      BUSINESS       0.48      0.39      0.43      1198\n",
      "       COLLEGE       0.49      0.41      0.45       229\n",
      "        COMEDY       0.57      0.34      0.43      1080\n",
      "         CRIME       0.46      0.48      0.47       712\n",
      "CULTURE & ARTS       0.54      0.20      0.29       215\n",
      "       DIVORCE       0.78      0.59      0.67       685\n",
      "     EDUCATION       0.43      0.30      0.35       203\n",
      " ENTERTAINMENT       0.57      0.71      0.63      3473\n",
      "   ENVIRONMENT       0.44      0.17      0.24       289\n",
      "         FIFTY       0.42      0.12      0.18       280\n",
      "  FOOD & DRINK       0.55      0.69      0.61      1268\n",
      "     GOOD NEWS       0.32      0.09      0.14       280\n",
      "         GREEN       0.41      0.32      0.36       524\n",
      "HEALTHY LIVING       0.33      0.11      0.16      1339\n",
      " HOME & LIVING       0.60      0.63      0.62       864\n",
      "        IMPACT       0.42      0.19      0.26       697\n",
      " LATINO VOICES       0.77      0.31      0.44       226\n",
      "         MEDIA       0.54      0.39      0.45       589\n",
      "         MONEY       0.46      0.29      0.35       351\n",
      "     PARENTING       0.47      0.58      0.52      1758\n",
      "       PARENTS       0.43      0.12      0.18       791\n",
      "      POLITICS       0.65      0.85      0.74      7121\n",
      "  QUEER VOICES       0.72      0.62      0.67      1269\n",
      "      RELIGION       0.58      0.49      0.53       515\n",
      "       SCIENCE       0.53      0.43      0.48       441\n",
      "        SPORTS       0.59      0.68      0.63      1015\n",
      "         STYLE       0.45      0.05      0.09       451\n",
      "STYLE & BEAUTY       0.62      0.76      0.68      1963\n",
      "         TASTE       0.29      0.07      0.12       419\n",
      "          TECH       0.53      0.44      0.48       421\n",
      " THE WORLDPOST       0.44      0.38      0.41       733\n",
      "        TRAVEL       0.57      0.73      0.64      1980\n",
      "     U.S. NEWS       0.47      0.03      0.05       275\n",
      "      WEDDINGS       0.75      0.74      0.75       731\n",
      "    WEIRD NEWS       0.32      0.14      0.19       555\n",
      "      WELLNESS       0.45      0.74      0.56      3589\n",
      "         WOMEN       0.44      0.25      0.32       714\n",
      "    WORLD NEWS       0.38      0.24      0.29       660\n",
      "     WORLDPOST       0.37      0.16      0.22       516\n",
      "\n",
      "      accuracy                           0.56     41906\n",
      "     macro avg       0.50      0.38      0.40     41906\n",
      "  weighted avg       0.54      0.56      0.52     41906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "# === NLTK setup ===\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# === Load JSON lines ===\n",
    "with open('news_categories.json', 'r') as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df = df[['headline', 'category']].dropna()\n",
    "\n",
    "# === Preprocessing Function ===\n",
    "ALLOWED_POS = {'NN', 'NNS', 'NNP', 'JJ', 'VB', 'VBD', 'VBG'}\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase and remove non-word characters\n",
    "    text = re.sub(r'\\W+', ' ', text.lower())\n",
    "\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Lemmatize and filter\n",
    "    clean_tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word, tag in tagged\n",
    "        if word.isalpha() and word not in stop_words and len(word) > 2 and tag in ALLOWED_POS\n",
    "    ]\n",
    "\n",
    "    return ' '.join(clean_tokens)\n",
    "\n",
    "# === Apply preprocessing ===\n",
    "df['clean'] = df['headline'].apply(preprocess)\n",
    "\n",
    "# === Encode labels ===\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['category'])\n",
    "\n",
    "# === Train-test split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['clean'], df['label'], test_size=0.2, stratify=df['label'], random_state=42\n",
    ")\n",
    "\n",
    "# === Classification pipeline ===\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        sublinear_tf=True,\n",
    "        norm='l2'\n",
    "    )),\n",
    "    ('clf', OneVsRestClassifier(\n",
    "        RidgeClassifier(alpha=1.0)\n",
    "    ))\n",
    "])\n",
    "\n",
    "# === Train and evaluate ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
