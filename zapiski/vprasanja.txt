1.natančnost klasifikacije se izračuna kot vsota pravilnih primerov (true positive, true negative) deljeno z številom vseh primerov, pove uspešnost našega sistema. senzitivnost pove koliko dejansko pozitivnih primerkov je bilo kvalificirano pravilno, pravilni deljeno z klasificiranimi pravilnimi (true positive, false negative). specifičnost je podobno kot senzitivnost vendar za negativne primerke, negativni deljeno z klasificiranimi negativnimi (true negative, false positive) 

2.bagging in boosting sta ansambelski metodi za klasifikacijo. uporabljata več različnih klasifikatorjev, ki glasujejo o končnem izidu. klasifikatorji morajo biti neodvisni in čim bolj raznoliki. bagging je tehnika boostrap vzorčenja. raznolikost zagotavlja preko naključnega vzorčenja objektov z vračanjem. za vsak klasifikator uporabimo 63% učne množice 1-1/e. prednost bagginga je, da je možna popolna paralelizacija.  boosting uporablja obteženo glasovanje klasifikatorjev. je iterativni pristop, pri katerem vsak naslednji klasifikator popravlja napake prejšnjega klasifikatorja. prednost boostinga je specializacija.

3.leno učenje je učenje na podlagi rešenih primerov. pri lenem učenju ni potrebna ponovna gradnja modela ob dodajanju novih primerkov. tako pridobimo hitrost pri gradnji modela, a izgubimo hitrost pri vračanju rezultatov. rezultat se snuje direktno iz učnih primerov, brez učenja. K-NN je tehnika na podlagi podobnosti objektov. vsak objekt je opisan z atributi, ki so predstavljene kot točke v 2D prostoru. ko dobimo nov objekt, poiščemo n najbližjih sosedov in glede na njih določimo kateri množici pripada. n se določa eksperimentalno, kadar imamo samo 2 množici naj bo n lih, da je vedno 1 rešitev.